{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ja0dmUxiac34FYmnm+iO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhamedani/Deep-learning-projects-Tensorflow/blob/main/8_fashion_mnist_hypertuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hypertuning\n",
        "Parameters such as weights and biases improve during training. But we have to determine the best hyperparameters ourselves. Such as **the number of hidden layers**, **batch-size**, **learning rate**, **the number of neurons in each layer**, **activate function**, and **epochs**."
      ],
      "metadata": {
        "id": "1GoTJNzCG7Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#hyperband\n",
        "There are several methods to determine the best hyperparameters, the best of which is hyperband, which we will review here"
      ],
      "metadata": {
        "id": "08oZHC-UIYcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install keras-tuner"
      ],
      "metadata": {
        "id": "3jLb8BCMBSQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JvYmRhHU_v5M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "obeiNZrWAzNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hyperparameters tune --> 1-number of neurons    2-learning rate"
      ],
      "metadata": {
        "id": "6N7hQN22GAfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):       # hp = hyperparameters # documentation https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "  #tune number of units\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32) # number of neurons = hp_units ,between 32 and 512 # type of data = int step = 32\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))\n",
        "  # tune learning rate can be 1e-2, 1e-3, 1e-4\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "1MS3dtW6Boek"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy', # val_accuracy  or val_loss determine measures for best hyperparameters\n",
        "                     max_epochs=10,    # maximum number of epochs for each hyperparameter combination\n",
        "                     factor=3, # factor = 3 means 3x3 = 9 hyperparameter combinations\n",
        "                     directory='my_dir', # for saving log\n",
        "                     project_name='reza')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KRHmijsCR17",
        "outputId": "6f0aa832-d8ab-4403-d855-ff677fc25f14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)  # stop training if no improvement in 5 epochs"
      ],
      "metadata": {
        "id": "bR9EINEtCgJH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early]) # search for best hyperparameters in 50 epochs (maximum)\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Learning Rate: {best_hps.get('learning_rate')}\")\n",
        "print(f\"Best Units: {best_hps.get('units')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ckCBa7CkWK",
        "outputId": "9a04339c-d9e3-4dd4-ee89-4d000de9e3dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate: 0.001\n",
            "Best Units: 352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyperparameters tune --> epoch"
      ],
      "metadata": {
        "id": "ZvMKm9VHLBaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc = history.history['val_accuracy']\n",
        "best_epoch = val_acc.index(max(val_acc)) + 1\n",
        "print(f\"Best epoch: {best_epoch}\")            # best epochs  = 18\n",
        "\n",
        "model = tuner.hypermodel.build(best_hps)      # fit model with best epochs=18  & best hyperparameters\n",
        "model.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "id": "Wxt9puijDIn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8gK8HiJEWJ9",
        "outputId": "fd0284df-a469-4de6-c732-4b3edb9e627c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8542 - loss: 0.4128\n",
            "[test loss, test accuracy]: [0.42475688457489014, 0.8493000268936157]\n"
          ]
        }
      ]
    }
  ]
}